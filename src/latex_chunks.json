[
    {
        "content": "\\documentclass[conference]{IEEEtran}\n%\\IEEEoverridecommandlockouts\n% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, p%lease comment it out.\n",
        "tag": "\\usepackage{subcaption}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{caption}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{listings}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{tikz}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{amsmath}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{multirow}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{tabulary}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{tikz,pgfplots,pgfplotstable,tikzscale,siunitx, xcolor}"
    },
    {
        "content": "\n",
        "tag": "\\usetikzlibrary{patterns}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{filecontents}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{graphicx}"
    },
    {
        "content": "\n\\usepackage[algo2e,ruled,vlined]{algorithm2e}\n",
        "tag": "\\usepackage{enumitem}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{stfloats}"
    },
    {
        "content": "\n",
        "tag": "\\usetikzlibrary{pgfplots.groupplots}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{cite}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{amsmath,amssymb,amsfonts}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{algorithmic}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{graphicx}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{textcomp}"
    },
    {
        "content": "\n",
        "tag": "\\usepackage{xcolor}"
    },
    {
        "content": "\n\\usepackage[breaklinks=true]{hyperref}\n\\def",
        "tag": "\\BibTeX{{\\rm B\\kern-.05em{\\sc i\\kern-.025em b}"
    },
    {
        "content": "\\kern-.08em\n    T\\kern-.1667em\\lower.7ex",
        "tag": "\\hbox{E}"
    },
    {
        "content": "\\kern-.125emX}}\n",
        "tag": "Outside"
    },
    {
        "content": "\n\n\\title{FPGA-based Hyrbid Memory Emulation System\n}\n\\author{\\IEEEauthorblockN{Fei Wen, Mian Qin, Paul V. Gratz, A.L.Narasimha Reddy}\n\\IEEEauthorblockA{\nDepartment of Electrical and Computer Engineering\\\\\nTexas A\\&M University\\\\\nCollege Station, TX 77843\\\\\n {\\tt \\{fwen, celery1124, pgratz, reddy\\}@tamu.edu}}}\n\\maketitle\n\\thispagestyle{plain}\n\\pagestyle{plain}\n  Hybrid memory systems, comprised of emerging non-volatile memory\n  (NVM) and DRAM, have been proposed to address the growing memory\n  demand of applications. Emerging NVM technologies, such as\n  phase-change memories (PCM), memristor, and 3D XPoint, have higher\n  capacity density, minimal static power consumption and lower cost\n  per GB. However, NVM has longer access latency and limited write\n  endurance as opposed to DRAM. The different characteristics of two\n  memory classes point towards the design of hybrid memory systems\n  containing multiple classes of main memory.\n  \n  In the iterative and incremental development of new architectures,\n  the timeliness of simulation completion is critical to project\n  progression.  Hence, a highly efficient simulation method is needed\n  to evaluate the performance of different hybrid memory system\n  designs.  Design exploration for hybrid memory systems is challenging, because it requires emulation of the\n  full system stack, including the OS, memory controller, and\n  interconnect. Moreover, benchmark applications for memory performance test typically have much larger working sets, thus taking even longer simulation warm-up period.\n  \n  In this paper, we propose a FPGA-based hybrid memory system\n  emulation platform.  We target at the mobile computing system, which\n  is sensitive to energy consumption and is likely to adopt NVM for\n  its power efficiency.  Here, because the focus of our platform is on\n  the design of the hybrid memory system, we leverage the on-board\n  hard IP ARM processors to both improve simulation performance while\n  improving accuracy of the results.  Thus, users can implement their\n  data placement/migration policies with the FPGA logic elements and\n  evaluate new designs quickly and effectively.  Results show that our\n  emulation platform provides a speedup of 9280x in simulation time\n  compared to the software counterpart Gem5.\n",
        "tag": "\\end{abstract}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "\nHardware emulation, FPGA accelerator, memory system, NVM\n",
        "tag": "\\end{IEEEkeywords}"
    },
    {
        "content": "\n",
        "tag": "\\section{Introduction}"
    },
    {
        "content": "\n",
        "tag": "\\label{sec:intro}"
    },
    {
        "content": "\nIn the era of big data, the memory footprint of applications has\nexpanded enormously. DRAM energy consumption grows linearly with its\ncapacity, as DRAM cells constantly draw energy to refresh its stored\ndata.  The non-volatile-memory (NVM) technologies, such as Intel 3D\nXpoint~",
        "tag": "\\cite{intel-3dxpoint}"
    },
    {
        "content": ", memristor~",
        "tag": "\\cite{memristor}"
    },
    {
        "content": ", and\nPhase-change-memory (PCM)~",
        "tag": "\\cite{PCM}"
    },
    {
        "content": " have emerged as alternative\nmemory technologies to DRAM.  These new memory technologies promise an\norder of magnitude higher density~",
        "tag": "\\cite{techinsights}"
    },
    {
        "content": " and minimal\nbackground power consumption.  Their access delay is typically within\none order of magnitude larger than that of DRAM while the write access\nhas significantly higher overheads.  \n",
        "tag": "Outside"
    },
    {
        "content": "[hbt]\n  \\centering\n  \\small\n\\caption{Approximate Performance Comparison of Different Memory\n  Technologies\\cite{NVM1,NVM2,yang:2012}{c|c|c|c|c|c|c}\n\\hline\nTechnology& HDD & FLASH & 3D XPoint & DRAM & STT-RAM & MRAM\\\\\n\\hline\nRead Latency & 5ms & $100\\mu s$  & 50 - 150ns & 50ns & 20ns & 20ns\\\\\nWrite Latency & 5ms & $100\\mu s$  & 50 - 500ns & 50ns & 20ns & 20ns\\\\\nEndurance (Cycles) & $>10^{15}$ & $10^{4}$ & $10^9$ & $>10^{16}$ & $>10^{16}$ & $>10^{15}$\\\\\n\\$ per GB & 0.025-0.5 & 0.25-0.83 & 6.5~\\cite{nvm_price} & 5.3-8 & N/A & N/A\\\\\nCell Size & N/A & $4-6F^2$ & $4.5F^2$ ~\\cite{techinsights} & $10F^2$ & $6-20F^2$ & $25F^2$\\\\\n\\hline \n",
        "tag": "\\end{tabular}"
    },
    {
        "content": "\n",
        "tag": "\\label{tab:nvms}"
    },
    {
        "content": "\nSuch different characteristics\nrequire a significant redesign of the memory system architecture, in\nboth data management policies and mechanisms.  Hybrid memory systems,\ncomprised of emerging non-volatile memory (NVM) and DRAM, in\nparticular, look promising for future system design.  As to date these\nnew NVM technologies are only beginning to become widely available,\nmany open questions have yet to be worked out in management policy and\nimplementation.  Evaluating many alternative approaches in hybrid\nmemory design is essential for future system design.\n\nEvaluating hybrid memory systems presents several unique challenges\nbecause we aim to test the whole system stack, comprising not only the\nCPU, but also the memory controller, memory devices and the\ninterconnections.  Further, since the study is focused on main memory,\naccurate modeling of DRAM is required.  Much of the prior work in the\nprocessor memory domain relies upon software simulation as the primary\nevaluation framework with tools such as Champsim~",
        "tag": "\\cite{champsim}"
    },
    {
        "content": " and\ngem5~",
        "tag": "\\cite{gem5}"
    },
    {
        "content": ". However, cycle-level software simulators that meets our requirement of system complexity and flexibility, impose huge simulation\ntime slow-downs versus real hardware. Moreover, there are often\nquestions of the degree of fidelity of the outcome of arbitrary\nadditions to software simulators~",
        "tag": "\\cite{7155440}"
    },
    {
        "content": ".\n\nIn this paper, we propose an evaluation system with the flexibility to\nallow the user to define their own hybrid memory management policy and\nwith the performance to run full applications at near native speed.\nIn particular our evaluation platform leverages an FPGA platform with\na hard IP ARM Cortex A57 processors and two memory controllers\nconnecting the FPGA to DRAM DIMMS.  This platform with our framework\nallows the hybrid memory management unit to be implemented in the FPGA\nin RTL, where the real application memory requests on the host ARM\ncores are redirected to memory DIMMs connected to the FPGA. Thus we\nsuccessfully decouple the design under test, i.e, the memory\nmanagement policies, from the detailed modelling of memory device. Our\nFPGA-based emulation platform provides the flexibility to develop and\ntest sophisticated memory management policies, while its hardware-like\nnature provides near-native simulation speed.\n\nIn this paper we detail the evaluation platform and compare its\nperformance to comparable simulation based approaches to hybrid memory\nmanagement simulation.  In particular, experimental results show\nthat our platform provides 2286x speedup compared to Champsim, and 9280x\nspeedup over Gem5.\n%Another research group has published their study on hybrid memory management~",
        "tag": "\\cite{ours}"
    },
    {
        "content": " based on our platform.\n",
        "tag": "\\section{Background and Motivation}"
    },
    {
        "content": "\nHybrid memories, comprised of DRAM and NVM, have recently become a hot\nresearch topic, with numerous works~",
        "tag": "\\cite{Hassan:2015,span,\n  Liu:2017,ramos, CSu}"
    },
    {
        "content": " in this domain.  For design evaluation, these\nworks predominantly use either software-based platform simulation,\nwith simulator runtime limiting the workloads that can be examined, or\nthey use analytical modeling, which has a large impact on accuracy.\n\nIn other domains, FPGA-accelerated simulators have demonstrated high\ntime efficiency and accuracy, far beyond the scope of typical\nsoftware-based alternatives.  Below we summarize the main advantages\nof FPGA-accelerated simulators and show how this approach can be\napplied to hybrid memory emulation system.\n\n%Chiou ",
        "tag": "\\emph{et al.}"
    },
    {
        "content": "~",
        "tag": "\\cite{Chiou:FAST}"
    },
    {
        "content": " proposed FAST, a full system, cycle accurate simulator which provide orders of magnitude speedup of simulation time compared to conventional software based simulators.  \n\n",
        "tag": "\\subsection{Speed and Accuracy}"
    },
    {
        "content": " \nFPGAs are composed of millions of lookup tables, each of them\nprogrammable to fulfill certain logic functions.  Owing to its\nhardware nature, FPGAs are great for parallel tasks and concurrent\nexecution.  Software-simulators, which are by nature sequentially\nexecuted, have hit the simulation wall; a phenomenon that the\nsimulation efficiency declines as the target hardware system becomes\nmore complex. While software-based simulators are easier to build,\nthey are slower, and less accurate. Therefore several prior works have\nturned to FPGA-accelerated simulators.\n\nChung ",
        "tag": "\\emph{et al.}"
    },
    {
        "content": "~",
        "tag": "\\cite{Chung:PROTOFLEX}"
    },
    {
        "content": " introduced Protoflex for\nmultiprocessor simulation, which greatly reduced the simulation\nduration for complex commercial applications.\nReSim~",
        "tag": "\\cite{Fytraki:ReSim}"
    },
    {
        "content": " is a trace-driven hardware simulator using\nFPGA. It improves the timing accuracy of simulation results, while\nlowering both hardware and time cost.\\par\nWhen it comes to studies on memory systems, the efficiency problem deteriorates for software-based simulation. First, the\ntraffic events in interconnections are highly unpredictable, thus the\nlatency shows a wide variability. In contrast, the CPU events can be\nmodeled by a fixed number of pipeline stages. Thus, prior\nworks~",
        "tag": "\\cite{Aport}"
    },
    {
        "content": " simply use a one-cycle ``magic\" memory to avoid\nmodeling a realistic memory hierarchy, due to the timing complexity.\n\\par\nMemory performance benchmark applications leverage large working sets to exert pressure on memory systems. Multi-GB data of the working set must be warmed up before we could exclude the transient performance fluctuation at cold start. Simulation acceleration techniques such as SimPoint and sampling, also require repetitive memory warm-up each time before we can obtain correct and representative sample. All these factors makes the simulation time prohibitive long with traditional software simulators. \nHence, an efficient FPGA-based emulation platform is highly desired\nfor studies on hybrid memory systems.\n\n",
        "tag": "\\subsection{Flexibility and Completeness}"
    },
    {
        "content": "\nWhile FPGA-based simulators are primarily recognized for their high\nsimulation efficiency and accuracy, they also provide unique scopes to\nother system metrics.  PrEsto~",
        "tag": "\\cite{Sunwoo:PrEsto}"
    },
    {
        "content": " is a simulator\ncreated by Sunwoo ",
        "tag": "\\emph{et al.}"
    },
    {
        "content": " for power consumption estimation. It's\nable to predict cycle-by-cycle power dissipation, at a speed several\norders of magnitude faster than the software-based competitors. Such\ndetailed dynamic power information is essential for modern processor\ndesigns.  ATLAS~",
        "tag": "\\cite{Njoroge:ATLAS}"
    },
    {
        "content": " is the first FPGA prototyping of\na chip-multiprocessors, with hardware-level support for transactional\nmemories.\n\nThe programmability of FPGA is also crucial to hybrid memory system\nemulation. Researchers are free to implement their own memory\nmanagement policies in the logic blocks. Further, they don't have to\ndeal with the modelling of memory device itself, as all memory\nrequests are automatically directed to the real memory DIMMs in our\nplatform.\n\nThe flexibility of FPGA also allow us to emulate various NMV\ntechnologies which has different access characteristic, as shown in\nTable~",
        "tag": "\\ref{tab:nvms}"
    },
    {
        "content": ". Our platform provides a simple way for\ncustomizing critical parameters such as latency.  FPGA also meets the\ncompleteness requirement for memory system evaluations: users can\neasily add a variety of performance counters of their choice. For\nexample, we implemented counters for read/write transactions to each\nmemory device respectively, and obtained a fairly accurate estimate of\nthe dynamic power consumption.\n",
        "tag": "\\section{Design}"
    },
    {
        "content": "\n",
        "tag": "\\label{sec:design}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "[h]\n\\centering\n\\hspace*{-0.2in}\n\\subfloat[Target System Architecture]{\\includegraphics[width=\\columnwidth]{figures/target.png}\n\\label{fig:target}}\n\\hfil\n\\hspace*{-0.2in}\n\\subfloat[Emulation System Architecture]{\\includegraphics[width=\\columnwidth]{figures/architecture.png}%\n\\label{fig:emulation}}\n\\caption{Target System and Emulation System Architecture}\n\\label{fig:architecture}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\nFigure~",
        "tag": "\\ref{fig:target}"
    },
    {
        "content": " shows the target system and emulation system of our proposed scheme. The target design is the Hybrid Meomory Manger Unit (HMMU) that will be integrated to the processor die. The HMMU directly manages two memory device and receives the memory requests from the host CPU after cache filtering. \nThe received memory requests are then processed based on the user-defined data placement policies, and forwarded correspondingly to either DRAM or NVM.  The HMMU also manages the data migration between DRAM and NVM, by controlling the high-bandwidth DMA engine that connects to the both memory devices.\\par\nIn the emulation system, as illustrated in Figure~",
        "tag": "\\ref{fig:emulation}"
    },
    {
        "content": ", the HMMU and DMA engine are implemented in FPGA, and connects to the host via PCIe links.\n\n",
        "tag": "\\subsection{Memory Request Processing Workflow}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "[h]\n\\centerline{\\includegraphics[width=0.8\\textwidth]{figures/flow.png}}\n\\caption{Request Processing Workflow}\n\\label{fig:flow}\n",
        "tag": "\\end{figure*}"
    },
    {
        "content": "\nFigure~",
        "tag": "\\ref{fig:flow}"
    },
    {
        "content": " briefly walks through the memory request processing workflow:\nPCIe hard IP block receives TLPs carrying the memory requests from the host CPU, and then forwards them to the RX module of the FPGA logic.\nRX Control module extracts the TLP header into the FIFO by the order they were received. Meanwhile the TLPs are also forwarded to the control logic which is highly pipelined. In the first stage, TLPs were decoded and the interpreted memory requests are populated into the following stages. Here, you can design your own memory management policies, which usually have three aspects: the memory access pattern recognition, data placement policy, and data migration policy.\nWe'd elaborate these policies in the later sections. The outcome of the control logic can also contain actions corresponding to these policies: \n",
        "tag": "Outside"
    },
    {
        "content": "\n    \\item Memory requests are forwarded to the memory controller (MC) of target device.\n    \\item Trigger data migration in the DMA.\n",
        "tag": "\\end{itemize}"
    },
    {
        "content": "\nThe journey ends for write memory request, when they arrive at the MC and the payload data were written into target memory device.\nAs for the read requests, the read back data are retrieved from memory, and returned in the same order as the requests were received.\nThe TX CNTL module receives the read back data from both NVM and DRAM, and assembles them with the corresponding header into a complete response message. The response messages are then encapsulated into PCIe transactional layer packets (TLP) at TX and then are sent back to the CPU host by the PCIe hard IP block.\n",
        "tag": "\\subsection{Heterogeneity Transparency}"
    },
    {
        "content": "\nAlthough hybrid memory is composed of two separate memory devices, designer might want to hide such heterogeneity from OS, and present one single flat memory space. Thus, all the data placement and migration between DRAM and NVM becomes transparent from the user applications' perspective. This mechanism is beneficial in several aspects:\n",
        "tag": "Outside"
    },
    {
        "content": "\n    \\item All the data movement are executed by purely hardware-based HMMU, without interrupting the OS and user applications.\n    \\item Programmers don't have to learn about data allocation among separate memory devices.\n    \\item Compatible with legacy applications.\n",
        "tag": "\\end{enumerate}"
    },
    {
        "content": "\nThere are many implementation methods to realize this design. A straightforward approach is to build another layer of address redirection table within the HMMU, where the physical address is translated to the actual memory device address. The mapping rule becomes part of the data placement policy.\n",
        "tag": "\\subsection{Memory Consistency}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "[h]\n\\centerline{\\includegraphics[width=\\columnwidth]{figures/consistency.png}}\n\\caption{Memory Consistency Risk}\n\\label{fig:consistency}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\n\nA risk of memory consistency error arises here as the memory requests are split into the two channels for NVM and DRAM, respectively. From the perspective of OS, there is only one single memory space and the OS cannot tell which memory device is the request for. Hence, a consistency error is possible as shown in Fig. ~",
        "tag": "\\ref{fig:consistency}"
    },
    {
        "content": ", when the data was not returned in the same order as it was requested. Such errors are prone to happen when the later request is aimed at DRAM, which has a lower access latency. We adopt a tag-matching mechanism to guarantee the consistency, while still allowing out-of-order memory media access for higher performance. In particular, we use the header information ,stored at ",
        "tag": "\\texttt{HDR FIFO}"
    },
    {
        "content": " in Fig.~",
        "tag": "\\ref{fig:flow}"
    },
    {
        "content": ", as the tag to save the order of memory requests.\n",
        "tag": "\\subsection{DMA Engine}"
    },
    {
        "content": "\nTo efficiently migrate data between DRAM and NVM, without interfering processor memory requests, we need to implement a dedicated DMA engine.\\par\nThe bit width of DMA shall be compliant with the memory controller for maximum throughput. Due to the unbalanced data rates of the two memory devices, and the discrepancy of clock frequencies between the DMA module and memory controllers, an internal buffer is needed to temporarily hold the data during migration. Besides the choice of these two primary design parameters, bit width and buffer size, a major concern is how to guarantee data coherence when a memory access conflicts with ongoing DMA data migration.\\par\nWhen DMA swaps two pages, the data is transferred in units of 512B-block.  We carefully designed the DMA so that it keeps track of the detailed page swap progress, to be specific, the address range of sub-blocks that have been transferred. When a memory request is targeted at the page being swapped, we use the swap progress indicator to decide where to redirect the memory requests. For instance, if it's a read request and the targeting cache-line falls behind the current swap progress, i.e., the requested data has been transferred to the new location, the DMA redirects the memory request to the destination device.  We spent considerable time to design and verify the logic design to ensure all possible cases are covered and processed properly.\n",
        "tag": "\\subsection{PCIe BAR Memory Mapping}"
    },
    {
        "content": "\nIn order to have the applications run on PCIe attached memory, we first need to map the memory space into system address space.\nPCIe devices have a set of Base Address Registers (BAR), which are assigned by the platform firmware during the booting process. These BARs carry the base address where the PCIe device is mapped in the system memory-mapped or IO-mapped space. Here we chose the memory-mapped mode as it supports prefetching on our PCIe attached memory. The size of each BAR was programmed into the PCIe IP before FPGA project compilation. \\par However, some embedded system might not have enough free system address space for our PCIe memories, which is usually larger than 2GB.\nHence we need to adjust the system mapping appropriately in the device tree of firmware image (e.g. U-boot), to reserve enough address range for the required sizes of PCIe devices. \n",
        "tag": "\\subsection{Arbitrary Latency Cycles}"
    },
    {
        "content": "\n",
        "tag": "\\label{mimic}"
    },
    {
        "content": "\nA great advantage of our platform is the flexibility of emulating different memory technologies, by adding arbitrary stall cycles to the access latency. This is essential when you don't have access to a real NVM DIMM, which is the case for many architecture research studies.\\par\nThere are multiple blocks in the workflow (Fig.~",
        "tag": "\\ref{fig:flow}"
    },
    {
        "content": ") where you can insert extra cycles of delay, e.g., between the control logic and MC, or on the data return path. Here we use 3D Xpoint as an example to show how we calculated the number of stalling cycles to be inserted.\\par \nWe measured the round trip time in FPGA cycles to access external DRAM\nDIMM first, and then scaled the number of stall cycles according to\nthe speed ratio between DRAM and 3D Xpoint, as described\nin the Table~",
        "tag": "\\ref{tab:nvms}"
    },
    {
        "content": ".Thus we have one DRAM DIMM running at\nfull speed and the other DRAM DIMM emulating the approximate speed of 3D XPoint. Hence the\nplatform is not constrained to any specific type of NVM, but rather\nallows us to study and compare the behaviors across any arbitrary\ncombinations of hybrid memories. \n%Since 3D Xpoint is the only cutting edge NVM technology that has been massively manufactured (Intel Optane series), we chose it for the simulation experiments presented in the following sections. \n",
        "tag": "\\subsection{Driver and Memory Allocator}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "[h]\n\\centerline{\\includegraphics[width=\\columnwidth]{figures/middle.png}}\n\\caption{Middleware}\n\\label{fig:middle}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\nThe PCIe BAR mapping renders the targeted hybrid memories as a continuous range in the system physical address space, which is presented as a device file ",
        "tag": "\\texttt{/dev/mem}"
    },
    {
        "content": ".\nTo mandate the application to only run on the hybrid memories, we need middleware as shown in Fig.~",
        "tag": "\\ref{fig:middle}"
    },
    {
        "content": ":\n",
        "tag": "Outside"
    },
    {
        "content": "\n    \\item The driver (\\texttt{mem\\_driver.ko}) manages the physical frames of the hybrid memories (\\texttt{/dev/mem}), with the help of the kernel's \\texttt{genpool} subsystem. It sets up the page table using the \\texttt{remap\\_pfn\\_range} kernel function.\n   \\item We modify the \\texttt{pages.c} of \\texttt{jemalloc} allocator~\\cite{jemalloc}, and use the \\texttt{mmap} function to enforce the application allocations within the address range of the specified device file (\\texttt{/dev/mem\\_driver}).\n",
        "tag": "\\end{enumerate}"
    },
    {
        "content": "\nUsers are free to implement their own optimized strategies of virtual memory management inside the allocator, or physical frames management inside the driver. For example, we extended the ",
        "tag": "\\texttt{malloc}"
    },
    {
        "content": " API, to accept users' hints of memory device preference regarding data placement, and populate these information through the stack to the hardware hybrid memory controller. \n",
        "tag": "\\pgfplotsset{\n    /pgfplots/ybar legend/.style={\n    /pgfplots/legend image code/.code={%\n       \\draw[##1,/tikz/.cd,yshift=-0.25em]\n        (0cm,0cm) rectangle (12pt, 1em);}"
    },
    {
        "content": ",\n   },\n}\n%Slowdown\n",
        "tag": "\\pgfplotstableread{\napplication\tfpga\tgem5\tchampsim\n500.perlbench\t1.951451546\t73839.82973\t16418.9097\n505.mcf\t15.36232386\t31079.28459\t4890.100918\n508.namd\t6.353220866\t49263.83823\t12972.40447\n519.lbm\t5.685779049\t39090.61064\t14428.22929\n520.omnetpp\t1.985561326\t10995.30433\t6418.788826\n523.xalancbmk\t4.900496468\t13177.75896\t6726.882922\n525.x264\t4.501875102\t18237.17124\t3877.859354\n531.deepsjeng\t1.663266195\t48400.49791\t9251.427302\n538.imagick\t1.166819817\t21169.82902\t4064.916096\n541.leela\t1.275545735\t35870.07482\t6850.625276\n544.nab\t1.175347568\t37875.96734\t8418.632861\n557.xz\t7.401716196\t25630.0926\t3809.654436\nGeomean\t3.167911073\t29397.82307\t7241.402645\n}"
    },
    {
        "content": "\\slowdown\n\n",
        "tag": "\\pgfplotstableread{\napplication\trds\twrs\n500.perlbench\t91269070848\t85151563776\n505.mcf\t3104449445888\t3103576014848\n508.namd\t8357298176\t8380891136\n519.lbm\t271550382080\t272163438592\n520.omnetpp\t257153286144\t250019299328\n523.xalancbmk\t693449916416\t696510857216\n525.x264\t39498645504\t39774453760\n531.deepsjeng\t61425254400\t58407272448\n538.imagick\t4803395584\t4827742208\n541.leela\t102015385600\t103786201088\n544.nab\t80701865984\t81461706752\n557.xz\t44052758528\t43823579136\n}"
    },
    {
        "content": "\\requests\n\n",
        "tag": "Outside"
    },
    {
        "content": "[\n        width=\\textwidth,\n        height=0.8\\columnwidth,\n        ymode=log,\n        ymin=1,\n        %yticklabel={\\pgfmathparse{\\tick*100}\\pgfmathprintnumber{\\pgfmathresult}\\%},\n        xtick = data,\n        xmin=-0.7, xmax=12.7,\n        xticklabels from table={\\slowdown}{application},\n        xticklabel style={font=\\small, rotate=45},\n        yticklabel style={font=\\small, rotate=45},\n        ybar,\n        bar width = 0.2cm,\n        legend style={at={(0.5,1)},font=\\normalsize,anchor=north,legend columns=3},\n      ]\n      \\addplot [fill=white]table [ x expr=\\coordindex, y expr=\\thisrow{fpga} ] {\\slowdown};\n      \\addplot [fill=lightgray]table [ x expr=\\coordindex, y expr=\\thisrow{champsim} ] {\\slowdown};\n      \\addplot [fill=black]table [ x expr=\\coordindex, y expr=\\thisrow{gem5}] {\\slowdown};\n            \\legend{FPGA, Champsim, Gem5}\n    ",
        "tag": "\\end{axis}"
    },
    {
        "content": "\n\n",
        "tag": "Outside"
    },
    {
        "content": "[\n        width=1.1\\columnwidth,\n        height=0.8\\columnwidth,\n        ymode=log,log basis y={2},\n        ylabel style={font=\\footnotesize},\n        yticklabel style={font=\\footnotesize},\n        xtick = data,\n        xmin=-0.75, xmax=11.75,\n        xticklabels from table={\\requests}{application},\n        xticklabel style={font=\\footnotesize, rotate=45, anchor=north east},\n        yticklabel style={font=\\small, rotate=45},\n        ybar,\n        bar width = 0.12cm,\n        legend style={at={(0.5,1)},font=\\normalsize,anchor=north,legend columns=3},\n      ]\n      \\addplot [fill=white]table [ x expr=\\coordindex, y expr=\\thisrow{rds} ] {\\requests};\n      \\addplot [fill=black]table [ x expr=\\coordindex, y expr=\\thisrow{wrs}] {\\requests};\n            \\legend{Read, Write}\n    ",
        "tag": "\\end{axis}"
    },
    {
        "content": "\n\n",
        "tag": "\\section{Evaluation}"
    },
    {
        "content": "\n",
        "tag": "\\label{sec:eval}"
    },
    {
        "content": "\nWe implemented an emulation platform that's targeted at the ARM-based mobile computing hybrid memory system. Mobile systems have limited energy budgets, and they're more sensitive to memory power consumption. Hence, we believe the NVM's advantage of minimal standby power will be highly valued.\\par \nIn this section,\n%we present the evaluation of our proposed FPGA-based hybrid memory emulation systems.  \nwe first present the hardware implementation, and the benchmark software applications. Then\nwe evaluate the performance of our proposed emulation scheme, by comparing against the two popular software-based simulators, namely Gem5~",
        "tag": "\\cite{gem5}"
    },
    {
        "content": " and Champsim~",
        "tag": "\\cite{champsim}"
    },
    {
        "content": ". Finally we analyze and discuss some of the\nmore interesting data points.\n\n",
        "tag": "\\subsection{Methodology}"
    },
    {
        "content": "\n",
        "tag": "\\label{sec:method}"
    },
    {
        "content": "\n",
        "tag": "\\subsubsection{Platform Implementation}"
    },
    {
        "content": "\nThe system implementation is illustrated in Figure~",
        "tag": "\\ref{fig:implementation}"
    },
    {
        "content": ".\nWe used the customized LS2085A board produced by Freescale which has 8 ARM Cortex A57 cores. The LS2085a connects to the FPGA board via a high-speed PCI Express 3.0 link, and manages the two\nmemory modules (DRAM and NVM) directly. The platform is physically set up as shown in Figure~",
        "tag": "\\ref{fig:boards}"
    },
    {
        "content": ".\n",
        "tag": "Outside"
    },
    {
        "content": "[!hbt]\n\\centerline{\\includegraphics[width=\\columnwidth]{figures/boards.jpg}}\n\\caption{Hardware Boards Setup}\n\\label{fig:boards}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\nSince 3D Xpoint is the only NVM technology that has turned into mass-production (Intel Optane series), we chose it for the presented experiments. We emulated the 3D XPoint device with regular DRAM DIMM, using the approach explained in~",
        "tag": "\\ref{mimic}"
    },
    {
        "content": ".\\par\nThe DRAM and NVM memories are\nmapped to the address range $[0x1240000000, 0x1288000000)$ of the physical memory space via the PCIe BAR (Base Address\nRegister) window. Note that the CPU caching is still enabled on the mapped memory space, due to the PCIe BAR configurations.\n",
        "tag": "Outside"
    },
    {
        "content": "[!hbt]\n\\centerline{\\includegraphics[width=\\columnwidth]{figures/implementation.png}}\n\\caption{System Implementation}\n\\label{fig:implementation}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\n\n\nThe\ndetailed system specification is listed in Table~",
        "tag": "\\ref{tab:System\n  Spec}"
    },
    {
        "content": ".\n",
        "tag": "Outside"
    },
    {
        "content": "[hpbt]\n\\centering\n\\settowidth\\tymin{\\textbf{Component}}\n\\caption{Emulation System Specification}{\\columnwidth}{L|L}\n  \\hline\n  {\\textbf{Component}} & Description \\\\\n  \\hline\n  CPU & ARM Cortex-A57 @ 2.0GHz, 8 cores, ARM v8 architecture\\\\\n  \\hline\n  L1 I-Cache & 48 KB instruction cache, 3-way set-associative\\\\\n  \\hline\n  L1 D-Cache & 32 KB data cache, 2-way set-associative\\\\\n  \\hline\n  L2 Cache & 1MB, 16-way associative, 64KB cache line size \\\\\n  \\hline\n  Interconnection & PCI Express Gen3 (8.0 Gbps) \\\\\n  \\hline\n   %\\multirow{2}{*}{Memory} & 128MB DDR4\\\\ & 1GB 3DXpoint(emulated by DDR4 with added latency) \\\\\n   DRAM & 128MB DDR4\\\\\n   \\hline\n   NVM & 1GB 3DXpoint(emulated by DDR4 with added latency) \\\\\n%\\hline\n%Emulated NVM Latency & Read 100ns / Write 400ns\\\\\n\\hline\nOS & Linux version 4.1.8 \\\\\n\\hline\n",
        "tag": "\\end{tabulary}"
    },
    {
        "content": "\n",
        "tag": "\\label{tab:System Spec}"
    },
    {
        "content": "\n\n\n\n\n\n",
        "tag": "\\subsubsection{Workloads}"
    },
    {
        "content": "\nWe choose applications from the latest version SPEC CPU 2017 benchmark suite~",
        "tag": "\\cite{SPEC_Official}"
    },
    {
        "content": ".  To emulate memory pressures of future mobile applications, we\nprefer those SPEC CPU 2017 benchmarks which require a larger\nworking set than the DRAM size in our system.  The details of tested benchmark applications are\nlisted in Table~",
        "tag": "\\ref{tab:Benchmarks}"
    },
    {
        "content": ".\n\n",
        "tag": "Outside"
    },
    {
        "content": "[hbt]\n\\centering\n\\settowidth\\tymin{\\textbf{Memory Footprint}}\n\\caption{Tested Workloads of SPEC 2017\\cite{SPEC_Official}} \\label{tab:Benchmarks}{\\columnwidth}{L|L|L}\n\\hline\n{\\textbf{Benchmark}} & Description & Memory footprint \\\\\n\\hline\n\\multicolumn {3}{c}{Integer Applications} \\\\\n\\hline\n500.perlbench\t& Perl interpreter & 202MB \\\\\n\\hline\n505.mcf & Vehicle route scheduling & 602MB\\\\\n\\hline\n508.namd & Molecular dynamics & 172MB \\\\\n\\hline\n520.omnetpp\t& Discrete Event simulation - computer network\t& 241MB \\\\\n\\hline\n523.xalancbmk\t& XML to HTML conversion via XSLT & 481MB \\\\\n\\hline\n525.x264\t& Video compressing & 165MB \\\\\n\\hline\n531.deepsjeng\t& Artificial Intelligence: alpha-beta tree search (Chess) & 700MB \\\\\n\\hline\n541.leela\t& AI: Monte Carlo tree search & 22MB \\\\\n\\hline\n557.xz & General data compression & 727MB \\\\\n\\hline\n\\multicolumn {3}{c}{Float Point Applications} \\\\\n\\hline\n519.lbm & Fluid dynamics & 410MB \\\\\n\\hline\n538.imagick & Image Manipulation & 287MB \\\\\n\\hline\n544.nab & Molecular Dynamics & 147MB \\\\\n\\hline\n",
        "tag": "\\end{tabulary}"
    },
    {
        "content": "\n\n",
        "tag": "\\subsubsection{Designs Under Test}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "\n    \\item Software-based simulators: Gem5, Champsim:\\par\n    We run the workloads~\\ref{tab:Benchmarks} in Gem5 (SE mode) and Champsim, on a x86 workstation with 2 Six-Core Intel Xeon Processor E5-2643 v3 (20M Cache, 3.40 GHz), and 128GB DDR4 Memory. \n    %We obtained the IPC by dividing the instruction counts against the simulation time in terms of CPU clock cycles.\n    We normalized the simulation time against the running time of native execution on the same machine.\n    \\item Our proposed FPGA-based emulation platform\\par\n    We run the same set of workloads on our emulation system~\\ref{tab:System Spec}, then we normalized the run time against the time taken by native execution.\n    Note that in native execution, the applications run in the on-board DDR4 (16GB) of the LS2085a board.\n",
        "tag": "\\end{itemize}"
    },
    {
        "content": "\n\n",
        "tag": "\\subsection{Results}"
    },
    {
        "content": "\n",
        "tag": "Outside"
    },
    {
        "content": "[htb]\n  \\centering\n  \\includegraphics{slowdown}\n  \\caption{Simulation Time Normalized against Native Execution}\n  \\label{fig:slowdown}\n",
        "tag": "\\end{figure*}"
    },
    {
        "content": "\n\n",
        "tag": "Outside"
    },
    {
        "content": "[htb]\n  \\centering\n  \\includegraphics{requests}\n  \\caption{Memory Requests (Bytes)}\n  \\label{fig:requests}\n",
        "tag": "\\end{figure}"
    },
    {
        "content": "\nFigure~",
        "tag": "\\ref{fig:slowdown}"
    },
    {
        "content": " shows the simulation time of each methods, normalized against the run time of native execution.\nThe geometric mean of slowdown factors indicates the overall time efficiency of each tested design (lower the better).\\par\nOur FPGA-based emulation system merely experienced 3.17x slowdown versus the native execution, while Gem5 showed 29397.82x slowdown, and Champsim yielded 7241.4x.\nAmong all the tested workloads, 538.imagick had smallest slowdown (1.17x), and 505.mcf suffered the largest performance penalty (15.36x).\\par\nTo investigate the memory behaviors of these applications, we implemented a variety of performance counters in the FPGA. We used one set of the counters to collect numbers of memory requests generated by each application, as presented in Figure~",
        "tag": "\\ref{fig:requests}"
    },
    {
        "content": ". We found that 538.imagick created the fewest memory requests(Read: 4.47GB, Write: 4.49GB); while 505.mcf incurred the most requests (Read: 2.83TB, Write: 2.82TB). Our observation is also confirmed by~",
        "tag": "\\cite{spec2017-workload}"
    },
    {
        "content": ", which reports that 505.mcf has the highest cache miss rate, and 538.imagic has the lowest L2/L3 cache miss rates among the tested workloads.\nBased on our experience developing the emulation system, we presume the major impact comes from the latency of the PCIe links. Consequently, applications with higher volume of memory accesses will be affected negatively. \n",
        "tag": "\\section{Conclusions}"
    },
    {
        "content": "\n",
        "tag": "\\label{sec:conc}"
    },
    {
        "content": "\nEmerging non-volatile memory (NVM) technologies provide higher\ncapacity and less static power consumption than traditional DRAM. Hybrid memories comprised of both NVM and DRAM\npromise to mitigate the energy limits on mobile computing/embedded system. The design space for hyrid memory management pollies are broad, with many\npossibilities for HW/SW cross-layer data allocation and migration.\nEvaluating hybrid memory systems requires full-stack system simulation involving bus/interconnection, memory controllers, memory devices, etc. With such level of complexity, cycle-level simulation becomes  prohibitive slow for traditional simulation-based techniques.\\par\n  Here we propose an FPGA based platform for rapid, accurate\nhybrid memory system evaluation.  In this platform, only the hybrid\nmemory management policies themselves are emulated in the FPGA\nhardware, the other system components (i.e. the processor, DRAM, etc)\nare all real components.  Thus users can focus on the core design and don't have to bother modelling system environments.\nExperiments on SPEC 2017 show that our design is 2286 times faster than Champsim, and 9280 times faster than Gem5. \\par\nThe flexibility of FPGA allow users to build performance counter of various functions, providing rich information and new scopes to design exploration.\n",
        "tag": "\\bibliographystyle{IEEEtran}"
    },
    {
        "content": "\n",
        "tag": "\\bibliography{refs.bib}"
    },
    {
        "content": "\n\n",
        "tag": "Remaining"
    }
]