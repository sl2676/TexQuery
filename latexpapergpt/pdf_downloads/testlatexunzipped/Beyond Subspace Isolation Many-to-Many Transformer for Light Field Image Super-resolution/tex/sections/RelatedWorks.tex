\section{Related Works}
\subsection{Single Image Super-resolution}
Single Image Super-resolution (SISR) is a classic low-level computer vision task aiming to reconstruct a high-resolution image (HR) from the low-resolution (LR) counterpart. Dong et al. \cite{dongSRCNN_ECCV2014} pioneered the introduction of CNN to this task, setting a new standard that outperformed previous SISR methods. This innovation marked the inception of a trend in the realm towards the widespread integration of deep neural networks. Subsequent achievements include VDSR \cite{kimVDSR_CVPR2016}, which leverages the residual connection to improve the data flow in a deep neural network; RDN \cite{zhangRDN_CVPR2018}, similarly improving the data flow via densely connected networks; and RCAN \cite{zhangRCAN_ECCV2018}, incorporating a residual-in-residual structure to further amplify the benefits of residual connections. Some works explored to utilize information in other domains for SISR, such as spectral information \cite{esmaeilzehi_TCI2021} and text-to-image models \cite{wuSeeSR_arXiv2023}.

Other contributions, such as SRGAN \cite{ledigSRGAN_CVPR2017} and EnhanceNet \cite{SajjadiEnhanceNet_ICCV2017}, emphasized the generation of visually appealing details by training networks using feature-based loss functions or adversarial learning. More recently, drawing inspiration from the success of Vision Transformer (ViT) \cite{ViT} in high-level vision tasks, Transformer-based SISR methods have further enhanced SISR by leveraging the self-attention mechanism. IPT \cite{chenIPT_CVPR2021} introduced image processing Transformers pre-trained across image processing tasks to benefit from datasets for not only SISR but also image denoizing and image restoration. SwinIR \cite{liangSwinIR_ICCV2021} introduced the Swin Transformer \cite{liuSwin_ICCV2021}, a shifted window scheme, to a series of low-level vision tasks. HAT \cite{chenHAT_CVPR2023} proposed a hybrid attention component that combines channel attention convolution and window-based Transformers to enable the capability of global statistics and local fitting. Despite their success, Transformers are inherently accompanied by a quadratic growth in computational complexity relative to the input image size, which remains a challenge to their applicability in SISR. In response to this challenge, studies such as SRFormer \cite{zhouSRFormer_ICCV2023} and ELAN \cite{zhangELAN_ECCV2022} have emerged, aiming to alleviate the computational burden. SRFormer \cite{zhouSRFormer_ICCV2023} achieved this through permuted self-attention, while ELAN \cite{zhangELAN_ECCV2022} employed a long-range attention mechanism.

Different from the sole focus of SISR on enhancing visual details destroyed in downsampling, the LFSR task aims not only to restore these details but also to maintain and improve the parallax structure across SAIs, enriching the realism of the resulting LF images.

\subsection{Light Field Image Super-resolution}
Processing 4D LF data presents significant challenges in developing neural networks. The application of 4D convolutions is a straightforward solution but results in computationally heavy models, making both training and inference difficult. To alleviate this drawback, Wang et al. \cite{wangLFRecognition_ECCV2016} introduced an interleaved filter as an approximation for light field material recognition. The filter decomposes a 4D decomposition into a spatial and angular convolution. They proved that comparable performance can be achieved by interleaving these two distinct convolutions. This decomposition scheme is adopted by Yoon et al. in LFCNN for LFSR \cite{yoon2017LFCNN}. LFCNN consists of a spatial sub-network for SAI processing and an angular sub-network composed of three branches to capture LF correlation in three different geometric directions. Yeung et al. \cite{yeungSAS_LFSR2019} proposed a deep neural network consisting of a series of spatial-angular separable (SAS) convolution, akin to interleaved filters but trained in an end-to-end manner. Jin et al. \cite{jinLFSSRATO_2020} proposed an all-to-one framework, where each SAI is individually super-resolved using the other SAIs. A structure-aware loss is incorporated to preserve LF images' inherent parallax structure. Wang et al. \cite{wangLfInterNet_ECCV2020} introduced a network to extract spatial and angular features in separate branches and iteratively fuse them. Liu et al. \cite{liuLFIINet_TMM2021} proposed a pyramid network with dilated convolutions to expand receptive fields in both spatial and angular subspaces. Chen et al. \cite{chen_TMM2021} incorporates the frequency domain and semantic prior and proposed a network to super-resolve both spatial and angular resolutions. Sun et al. \cite{sun_TMM2022} proposed an network with disparity-exploited and non-disparity branches to learn a compact spatial-angular representation. Hu et al.\cite{huDKNet_TIM2022} proposed Decomposition Kernel Network (DKNet), which generalizes the decomposition scheme to comprehensively cover the spatial, angular and EPI subspaces. Further advancing the scheme, Wang et al. \cite{wangDistgSSR_TIP2022} proposed a disentangling mechanism to enhance the effectiveness. Different from the previous methods, some works resorts to non-deep-learning-based models \cite{rossi2018geometry, ghassab_TMM2019}. Some works explored plug-and-play strategies to boost the performance of existing methods, like the learning prior from single images \cite{wangBoosting_TCI2023} and the cut-and-blend data augmentation \cite{xiaoCutMIB_CVPR2023}.

In parallel to SISR, ViT has broadened the LFSR landscape. DPT \cite{wangDPT_AAAI2022} leveraged Transformers to learn image and gradient information among SAIs in horizontal and vertical sequences. LFT \cite{liangLFT_SPL2022} drew parallels with the earlier decomposition scheme but employed Transformers in place of separable convolutions. To enable spatial Transformers to model both local and non-local dependencies, spatial features were locally unfolded into patches and subsequently processed through a linear layer for local embedding before self-attention. EPIT proposed to utilize Transformers in horizontal and vertical EPI subspaces. Liang et al. proposed EPIT \cite{liangEPIT_arXiv2023} to further explore the use of Transformers in horizontal and vertical EPI subspaces. 

Despite these advancements, a common limitation predominantly persists across the most decomposition-based methods: subspace isolation, as elaborated in Section \ref*{section:Introduction}. This limitation motivates the derivation of our work in this paper.
